{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plantilla para la Tarea online BDA02\n",
    "\n",
    "# David Carlón Cembranos\n",
    "\n",
    "En esta tarea deberás completar las celdas que están incompletas. Se muestra el resultado esperado de la ejecución. Se trata de que implementes un proceso MapReduce que produzca ese resultado. Puedes implementar el proceso MapReduce con el lenguaje y librería que prefieras (`Bash`, Python, `mrjob` ...). Los datos de entrada del proceso son meros ejemplos y el proceso que implementes debería funcionar con esos y cualquier otro fichero de entrada que tenga la misma estructura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Partiendo del fichero de `notas.txt`, calcula la nota más alta obtenida por cada alumno con un proceso MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es decir, que si tenemos el fichero de notas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting notas.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile notas.txt\n",
    "pedro 6 7\n",
    "luis 0 4\n",
    "ana 7\n",
    "pedro 8 1 3\n",
    "ana 5 6 7\n",
    "ana 10\n",
    "luis 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se espera obtener el siguiente resultado:\n",
    "\n",
    "![solución 1](./img/1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting marksMR.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile marksMR.py\n",
    "#!/usr/bin/python3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "#Definimos una clase MrJob\n",
    "class MarksMR(MRJob):\n",
    "        \n",
    "    # Mapper: En esta etapa aún no hay clave (_), el valor lo recibimos en la variable line\n",
    "    def mapper(self, _, line):\n",
    "        #Por cada línea, esta se divide en los campos que forman las columnas\n",
    "        name, *marks = line.split()\n",
    "        for mark in marks:            \n",
    "            yield name, float(mark)\n",
    "         \n",
    "    #Reducer: La clave será el nombre y los valores las notas\n",
    "    def reducer(self, name, marks):\n",
    "        yield name, max(marks)\n",
    "        \n",
    "if __name__=='__main__':\n",
    "    MarksMR.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod ugo+x marksMR.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/marksMR.root.20240120.120444.034067\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/marksMR.root.20240120.120444.034067/output\n",
      "Streaming final output from /tmp/marksMR.root.20240120.120444.034067/output...\n",
      "\"ana\"\t10.0\n",
      "\"luis\"\t4.0\n",
      "\"pedro\"\t8.0\n",
      "Removing temp directory /tmp/marksMR.root.20240120.120444.034067...\n"
     ]
    }
   ],
   "source": [
    "! python3 marksMR.py notas.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -copyFromLocal notas.txt /user/root/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /app/hadoop-3.3.1/bin...\n",
      "Found hadoop binary: /app/hadoop-3.3.1/bin/hadoop\n",
      "Using Hadoop version 3.3.1\n",
      "Looking for Hadoop streaming jar in /app/hadoop-3.3.1...\n",
      "Found Hadoop streaming jar: /app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar\n",
      "Creating temp directory /tmp/marksMR.root.20240120.121356.073349\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/marksMR.root.20240120.121356.073349/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/marksMR.root.20240120.121356.073349/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar4757324624452854386/] [] /tmp/streamjob1839655337540389898.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.21.0.3:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.21.0.3:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1705747935772_0009\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1705747935772_0009\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1705747935772_0009\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1705747935772_0009/\n",
      "  Running job: job_1705747935772_0009\n",
      "  Job job_1705747935772_0009 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1705747935772_0009 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/marksMR.root.20240120.121356.073349/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=92\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=34\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=176\n",
      "\t\tFILE: Number of bytes written=834441\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=276\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=34\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3118080\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1409024\n",
      "\t\tTotal time spent by all map tasks (ms)=3045\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3045\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1376\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1376\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3045\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1376\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=830\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=88\n",
      "\t\tInput split bytes=184\n",
      "\t\tMap input records=7\n",
      "\t\tMap output bytes=144\n",
      "\t\tMap output materialized bytes=182\n",
      "\t\tMap output records=13\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=348041216\n",
      "\t\tPeak Map Virtual memory (bytes)=2566713344\n",
      "\t\tPeak Reduce Physical memory (bytes)=205225984\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2574331904\n",
      "\t\tPhysical memory (bytes) snapshot=859213824\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce input records=13\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=182\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=26\n",
      "\t\tTotal committed heap usage (bytes)=866123776\n",
      "\t\tVirtual memory (bytes) snapshot=7706791936\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/marksMR.root.20240120.121356.073349/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/marksMR.root.20240120.121356.073349/output...\n",
      "\"ana\"\t10.0\n",
      "\"luis\"\t4.0\n",
      "\"pedro\"\t8.0\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/marksMR.root.20240120.121356.073349...\n",
      "Removing temp directory /tmp/marksMR.root.20240120.121356.073349...\n"
     ]
    }
   ],
   "source": [
    "! python3 marksMR.py -r hadoop hdfs:///user/root/notas.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- Usando un proceso MapReduce muestra las 10 palabras más utilizadas en `El Quijote`.\n",
    "\n",
    "Lo primero será descargar El Quijote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-17 19:46:36--  https://www.gutenberg.org/files/2000/2000-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2226045 (2.1M) [text/plain]\n",
      "Saving to: ‘2000-0.txt’\n",
      "\n",
      "2000-0.txt          100%[===================>]   2.12M  2.40MB/s    in 0.9s    \n",
      "\n",
      "2024-01-17 19:46:37 (2.40 MB/s) - ‘2000-0.txt’ saved [2226045/2226045]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -O '2000-0.txt' https://www.gutenberg.org/files/2000/2000-0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que hicimos en la primera práctica, eliminamos aquellas líneas que son metadata y no forman parte de la obra. Sobrescribimos el fichero sin esas líneas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2000-0.txt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "head = 24\n",
    "tail = 360\n",
    "book = lines[head:-tail]\n",
    "\n",
    "with open('2000-0.txt', 'w') as f:\n",
    "    for line in book:\n",
    "        f.write(f\"{line}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado debería ser el mismo que el que obtuvimos en la primera práctica.\n",
    "\n",
    "![solución 2](./img/2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting most_used_words.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile most_used_words.py\n",
    "#!/usr/bin/python3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "WORD_PATTERN = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class MRCommonWord(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.map_words,\n",
    "                   combiner=self.combine_counts,\n",
    "                   reducer=self.reduce_counts),\n",
    "            MRStep(reducer=self.reduce_frequency)\n",
    "        ]\n",
    "\n",
    "    def map_words(self, _, line):\n",
    "        for word in WORD_PATTERN.findall(line):\n",
    "            yield (word.lower(), 1)\n",
    "\n",
    "    def combine_counts(self, word, counts):\n",
    "        yield (word, sum(counts))\n",
    "\n",
    "    def reduce_counts(self, word, counts):\n",
    "        yield None, (sum(counts), word)\n",
    "        \n",
    "    def reduce_frequency(self, _, word_counts):\n",
    "        sorted_counts = sorted(word_counts, key=lambda x: x[0], reverse=True)\n",
    "        yield None, list(sorted_counts[:10])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRCommonWord.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod ugo+x most_used_words.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/most_used_words.root.20240120.112256.574476\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/most_used_words.root.20240120.112256.574476/output\n",
      "Streaming final output from /tmp/most_used_words.root.20240120.112256.574476/output...\n",
      "null\t[[20651, \"que\"], [18276, \"de\"], [18150, \"y\"], [10433, \"la\"], [9816, \"a\"], [8236, \"en\"], [8204, \"el\"], [6304, \"no\"], [4737, \"los\"], [4723, \"se\"]]\n",
      "Removing temp directory /tmp/most_used_words.root.20240120.112256.574476...\n"
     ]
    }
   ],
   "source": [
    "! python3 most_used_words.py 2000-0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -copyFromLocal 2000-0.txt /user/root/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /app/hadoop-3.3.1/bin...\n",
      "Found hadoop binary: /app/hadoop-3.3.1/bin/hadoop\n",
      "Using Hadoop version 3.3.1\n",
      "Looking for Hadoop streaming jar in /app/hadoop-3.3.1...\n",
      "Found Hadoop streaming jar: /app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar\n",
      "Creating temp directory /tmp/most_used_words.root.20240120.112311.453859\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/most_used_words.root.20240120.112311.453859/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/most_used_words.root.20240120.112311.453859/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar7679586938635845343/] [] /tmp/streamjob3618276325474358801.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.21.0.3:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.21.0.3:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1705747935772_0003\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1705747935772_0003\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1705747935772_0003\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1705747935772_0003/\n",
      "  Running job: job_1705747935772_0003\n",
      "  Job job_1705747935772_0003 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1705747935772_0003 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/most_used_words.root.20240120.112311.453859/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2419426\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=512633\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=500469\n",
      "\t\tFILE: Number of bytes written=1836698\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2419612\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=512633\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=6384640\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2060288\n",
      "\t\tTotal time spent by all map tasks (ms)=6235\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6235\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2012\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2012\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=6235\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2012\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2770\n",
      "\t\tCombine input records=381159\n",
      "\t\tCombine output records=31379\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=72\n",
      "\t\tInput split bytes=186\n",
      "\t\tMap input records=299120\n",
      "\t\tMap output bytes=3753955\n",
      "\t\tMap output materialized bytes=500475\n",
      "\t\tMap output records=381159\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=308469760\n",
      "\t\tPeak Map Virtual memory (bytes)=2566664192\n",
      "\t\tPeak Reduce Physical memory (bytes)=206102528\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2570334208\n",
      "\t\tPhysical memory (bytes) snapshot=822325248\n",
      "\t\tReduce input groups=23070\n",
      "\t\tReduce input records=31379\n",
      "\t\tReduce output records=23070\n",
      "\t\tReduce shuffle bytes=500475\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=62758\n",
      "\t\tTotal committed heap usage (bytes)=842006528\n",
      "\t\tVirtual memory (bytes) snapshot=7703027712\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar2769652280536153251/] [] /tmp/streamjob1365600744109301351.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.21.0.3:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.21.0.3:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1705747935772_0004\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1705747935772_0004\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1705747935772_0004\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1705747935772_0004/\n",
      "  Running job: job_1705747935772_0004\n",
      "  Job job_1705747935772_0004 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1705747935772_0004 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/most_used_words.root.20240120.112311.453859/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=516729\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=150\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=558779\n",
      "\t\tFILE: Number of bytes written=1951926\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=517057\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=150\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3081216\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1520640\n",
      "\t\tTotal time spent by all map tasks (ms)=3009\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3009\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1485\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1485\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3009\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1485\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1660\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=69\n",
      "\t\tInput split bytes=328\n",
      "\t\tMap input records=23070\n",
      "\t\tMap output bytes=512633\n",
      "\t\tMap output materialized bytes=558785\n",
      "\t\tMap output records=23070\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=307220480\n",
      "\t\tPeak Map Virtual memory (bytes)=2568626176\n",
      "\t\tPeak Reduce Physical memory (bytes)=206057472\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2570252288\n",
      "\t\tPhysical memory (bytes) snapshot=818827264\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=23070\n",
      "\t\tReduce output records=1\n",
      "\t\tReduce shuffle bytes=558785\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=46140\n",
      "\t\tTotal committed heap usage (bytes)=846200832\n",
      "\t\tVirtual memory (bytes) snapshot=7705313280\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/most_used_words.root.20240120.112311.453859/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/most_used_words.root.20240120.112311.453859/output...\n",
      "null\t[[20651, \"que\"], [18276, \"de\"], [18150, \"y\"], [10433, \"la\"], [9816, \"a\"], [8236, \"en\"], [8204, \"el\"], [6304, \"no\"], [4737, \"los\"], [4723, \"se\"]]\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/most_used_words.root.20240120.112311.453859...\n",
      "Removing temp directory /tmp/most_used_words.root.20240120.112311.453859...\n"
     ]
    }
   ],
   "source": [
    "! python3 most_used_words.py -r hadoop hdfs:///user/root/2000-0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.- Muestra la clasificación de temporada 2021/2022 de La Liga pero únicamente de los puntos obtenidos como visitante.\n",
    "\n",
    "En [esta Web](https://resultados.as.com/resultados/futbol/primera/2021_2022/clasificacion/) puedes consultar cuántos puntos obtuvo cada equipo fuera de casa.\n",
    "\n",
    "Empezamos descargando el fichero de resultados de la temporada 2021/2022 y renombrándolo a `laliga2122.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-12-05 10:25:13--  https://www.football-data.co.uk/mmz4281/2122/SP1.csv\n",
      "Resolving www.football-data.co.uk (www.football-data.co.uk)... 217.160.0.246\n",
      "Connecting to www.football-data.co.uk (www.football-data.co.uk)|217.160.0.246|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 172174 (168K) [text/csv]\n",
      "Saving to: ‘laliga2122.csv’\n",
      "\n",
      "laliga2122.csv      100%[===================>] 168.14K   625KB/s    in 0.3s    \n",
      "\n",
      "2022-12-05 10:25:14 (625 KB/s) - ‘laliga2122.csv’ saved [172174/172174]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -O laliga2122.csv https://www.football-data.co.uk/mmz4281/2122/SP1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se espera este resultado:\n",
    "\n",
    "![solución 3](./img/3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ligavisitante.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ligavisitante.py\n",
    "#!/usr/bin/python3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "    \n",
    "class LaLigaMR(MRJob):\n",
    "        \n",
    "    # Mapper: En esta etapa aún no hay clave (_), el valor lo recibimos en la variable line\n",
    "    def mapper_points(self, _, line):\n",
    "        #Por cada línea, esta se divide en los campos que forman las columnas\n",
    "        _, _, _, home_team, away_team, _, _, result, *rest = line.split(',')\n",
    "        \n",
    "\n",
    "        if home_team == \"HomeTeam\":\n",
    "            return\n",
    "        \n",
    "\n",
    "        if result == 'D':            \n",
    "            yield away_team, 1\n",
    "        elif result == 'A':\n",
    "            yield away_team, 3\n",
    "            \n",
    "    def combiner_points(self, team, points):\n",
    "        yield team, sum(points)\n",
    "            \n",
    "    def reducer_points(self, team, points):\n",
    "        yield None, (team, sum(points))\n",
    "        \n",
    "    def reducer_classification(self, _, points):\n",
    "        yield None, sorted(points, key=lambda t: t[1], reverse=True)\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_points,\n",
    "                   combiner=self.combiner_points,\n",
    "                   reducer=self.reducer_points),\n",
    "            MRStep(reducer=self.reducer_classification)\n",
    "        ]\n",
    "         \n",
    "if __name__=='__main__':\n",
    "    LaLigaMR.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/ligavisitante.root.20240120.105717.637809\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/ligavisitante.root.20240120.105717.637809/output\n",
      "Streaming final output from /tmp/ligavisitante.root.20240120.105717.637809/output...\n",
      "null\t[[\"Real Madrid\", 42], [\"Barcelona\", 35], [\"Betis\", 33], [\"Ath Madrid\", 30], [\"Sevilla\", 28], [\"Sociedad\", 27], [\"Osasuna\", 25], [\"Villarreal\", 23], [\"Valencia\", 22], [\"Ath Bilbao\", 21], [\"Celta\", 21], [\"Cadiz\", 21], [\"Granada\", 16], [\"Elche\", 15], [\"Levante\", 13], [\"Vallecano\", 13], [\"Mallorca\", 12], [\"Getafe\", 11], [\"Espanol\", 9], [\"Alaves\", 6]]\n",
      "Removing temp directory /tmp/ligavisitante.root.20240120.105717.637809...\n"
     ]
    }
   ],
   "source": [
    "! python3 ligavisitante.py laliga2122.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /app/hadoop-3.3.1/bin...\n",
      "Found hadoop binary: /app/hadoop-3.3.1/bin/hadoop\n",
      "Using Hadoop version 3.3.1\n",
      "Looking for Hadoop streaming jar in /app/hadoop-3.3.1...\n",
      "Found Hadoop streaming jar: /app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar\n",
      "Creating temp directory /tmp/ligavisitante.root.20240120.105742.374952\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/ligavisitante.root.20240120.105742.374952/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/ligavisitante.root.20240120.105742.374952/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar3080984501314666200/] [] /tmp/streamjob3575331902094351241.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.21.0.3:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.21.0.3:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1705747935772_0001\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1705747935772_0001\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1705747935772_0001\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1705747935772_0001/\n",
      "  Running job: job_1705747935772_0001\n",
      "  Job job_1705747935772_0001 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1705747935772_0001 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/ligavisitante.root.20240120.105742.374952/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=176270\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=428\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=608\n",
      "\t\tFILE: Number of bytes written=837075\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=176580\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=428\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3250176\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1409024\n",
      "\t\tTotal time spent by all map tasks (ms)=3174\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3174\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1376\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1376\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3174\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1376\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=870\n",
      "\t\tCombine input records=215\n",
      "\t\tCombine output records=40\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=77\n",
      "\t\tInput split bytes=310\n",
      "\t\tMap input records=381\n",
      "\t\tMap output bytes=2730\n",
      "\t\tMap output materialized bytes=614\n",
      "\t\tMap output records=215\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=307281920\n",
      "\t\tPeak Map Virtual memory (bytes)=2569981952\n",
      "\t\tPeak Reduce Physical memory (bytes)=207790080\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2571075584\n",
      "\t\tPhysical memory (bytes) snapshot=821600256\n",
      "\t\tReduce input groups=20\n",
      "\t\tReduce input records=40\n",
      "\t\tReduce output records=20\n",
      "\t\tReduce shuffle bytes=614\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=80\n",
      "\t\tTotal committed heap usage (bytes)=842530816\n",
      "\t\tVirtual memory (bytes) snapshot=7707402240\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar4624104128991010652/] [] /tmp/streamjob2089785186292732888.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.21.0.3:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.21.0.3:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1705747935772_0002\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1705747935772_0002\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1705747935772_0002\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1705747935772_0002/\n",
      "  Running job: job_1705747935772_0002\n",
      "  Job job_1705747935772_0002 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1705747935772_0002 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/ligavisitante.root.20240120.105742.374952/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=642\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=354\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=474\n",
      "\t\tFILE: Number of bytes written=835235\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=966\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=354\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=2812928\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1846272\n",
      "\t\tTotal time spent by all map tasks (ms)=2747\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=2747\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1803\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1803\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=2747\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1803\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=850\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=67\n",
      "\t\tInput split bytes=324\n",
      "\t\tMap input records=20\n",
      "\t\tMap output bytes=428\n",
      "\t\tMap output materialized bytes=480\n",
      "\t\tMap output records=20\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=308264960\n",
      "\t\tPeak Map Virtual memory (bytes)=2565685248\n",
      "\t\tPeak Reduce Physical memory (bytes)=198447104\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2570424320\n",
      "\t\tPhysical memory (bytes) snapshot=811134976\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=20\n",
      "\t\tReduce output records=1\n",
      "\t\tReduce shuffle bytes=480\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=40\n",
      "\t\tTotal committed heap usage (bytes)=843579392\n",
      "\t\tVirtual memory (bytes) snapshot=7701217280\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/ligavisitante.root.20240120.105742.374952/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/ligavisitante.root.20240120.105742.374952/output...\n",
      "null\t[[\"Real Madrid\", 42], [\"Barcelona\", 35], [\"Betis\", 33], [\"Ath Madrid\", 30], [\"Sevilla\", 28], [\"Sociedad\", 27], [\"Osasuna\", 25], [\"Villarreal\", 23], [\"Valencia\", 22], [\"Celta\", 21], [\"Cadiz\", 21], [\"Ath Bilbao\", 21], [\"Granada\", 16], [\"Elche\", 15], [\"Vallecano\", 13], [\"Levante\", 13], [\"Mallorca\", 12], [\"Getafe\", 11], [\"Espanol\", 9], [\"Alaves\", 6]]\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/ligavisitante.root.20240120.105742.374952...\n",
      "Removing temp directory /tmp/ligavisitante.root.20240120.105742.374952...\n"
     ]
    }
   ],
   "source": [
    "! python3 ligavisitante.py -r hadoop laliga2122.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.- Muestra la diferencia de goles entre el equipo que más goles ha marcado y el que menos goles ha marcado en la temporada 2021/2022 de La Liga.\n",
    "\n",
    "Se espera que el proceso MapReuce produzca una salida similar a la siguiente:\n",
    "\n",
    "![solución 4](./img/4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ligagoals.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ligagoals.py\n",
    "#!/usr/bin/python3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class LaLigaAnalysis(MRJob):\n",
    "\n",
    "    def mapper_score(self, _, line):\n",
    "        _, _, _, home_team, away_team, home_goals, away_goals, result, *rest = line.split(',')\n",
    "        \n",
    "        if home_team == \"HomeTeam\":\n",
    "            return\n",
    "\n",
    "        if result in ['D', 'H']: \n",
    "            yield home_team, (int(home_goals))\n",
    "            yield away_team,(int(away_goals))\n",
    "        else:            \n",
    "            yield away_team, (int(away_goals))\n",
    "            yield home_team,(int(home_goals))\n",
    "\n",
    "    def combiner_score(self, team, goals):\n",
    "        yield team, sum(goals)\n",
    "            \n",
    "    def reducer_score(self, team, goals):\n",
    "        yield None, (team, sum(goals))\n",
    "        \n",
    "    def reducer_score_diff(self, _, teams):\n",
    "        sorted_teams = sorted(teams, key=lambda t: t[1], reverse=True)\n",
    "        top_team = sorted_teams[0][0]\n",
    "        bottom_team = sorted_teams[-1][0]\n",
    "        score_diff = sorted_teams[0][1] - sorted_teams[-1][1]\n",
    "        \n",
    "        match_up = top_team + \" vs \" + bottom_team\n",
    "        diff_str = \"Diferencia de goles \" + str(score_diff)\n",
    "        \n",
    "        yield  (match_up , diff_str)\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_score,\n",
    "                   combiner=self.combiner_score,\n",
    "                   reducer=self.reducer_score),\n",
    "            MRStep(reducer=self.reducer_score_diff)\n",
    "        ]\n",
    "\n",
    "if __name__=='__main__':\n",
    "    LaLigaAnalysis.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/ligagoals.root.20240120.112639.033178\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/ligagoals.root.20240120.112639.033178/output\n",
      "Streaming final output from /tmp/ligagoals.root.20240120.112639.033178/output...\n",
      "\"Real Madrid vs Alaves\"\t\"Diferencia de goles 49\"\n",
      "Removing temp directory /tmp/ligagoals.root.20240120.112639.033178...\n"
     ]
    }
   ],
   "source": [
    "! python3 ligagoals.py laliga2122.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /app/hadoop-3.3.1/bin...\n",
      "Found hadoop binary: /app/hadoop-3.3.1/bin/hadoop\n",
      "Using Hadoop version 3.3.1\n",
      "Looking for Hadoop streaming jar in /app/hadoop-3.3.1...\n",
      "Found Hadoop streaming jar: /app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar\n",
      "Creating temp directory /tmp/ligagoals.root.20240120.112710.268554\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/ligagoals.root.20240120.112710.268554/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/ligagoals.root.20240120.112710.268554/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar137519969768994010/] [] /tmp/streamjob1411855678605371120.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.21.0.3:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.21.0.3:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1705747935772_0005\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1705747935772_0005\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1705747935772_0005\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1705747935772_0005/\n",
      "  Running job: job_1705747935772_0005\n",
      "  Job job_1705747935772_0005 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1705747935772_0005 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/ligagoals.root.20240120.112710.268554/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=176270\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=430\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=626\n",
      "\t\tFILE: Number of bytes written=836919\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=176572\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=430\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3359744\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1414144\n",
      "\t\tTotal time spent by all map tasks (ms)=3281\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3281\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1381\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1381\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3281\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1381\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=910\n",
      "\t\tCombine input records=760\n",
      "\t\tCombine output records=40\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=71\n",
      "\t\tInput split bytes=302\n",
      "\t\tMap input records=381\n",
      "\t\tMap output bytes=9500\n",
      "\t\tMap output materialized bytes=632\n",
      "\t\tMap output records=760\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=311328768\n",
      "\t\tPeak Map Virtual memory (bytes)=2566422528\n",
      "\t\tPeak Reduce Physical memory (bytes)=202190848\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2571272192\n",
      "\t\tPhysical memory (bytes) snapshot=820994048\n",
      "\t\tReduce input groups=20\n",
      "\t\tReduce input records=40\n",
      "\t\tReduce output records=20\n",
      "\t\tReduce shuffle bytes=632\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=80\n",
      "\t\tTotal committed heap usage (bytes)=843579392\n",
      "\t\tVirtual memory (bytes) snapshot=7704010752\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar6437778996346905972/] [] /tmp/streamjob703959711133891740.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.21.0.3:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.21.0.3:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1705747935772_0006\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1705747935772_0006\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1705747935772_0006\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1705747935772_0006/\n",
      "  Running job: job_1705747935772_0006\n",
      "  Job job_1705747935772_0006 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1705747935772_0006 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/ligagoals.root.20240120.112710.268554/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=645\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=476\n",
      "\t\tFILE: Number of bytes written=835068\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=961\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=49\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=2964480\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1418240\n",
      "\t\tTotal time spent by all map tasks (ms)=2895\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=2895\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1385\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1385\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=2895\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1385\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=800\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=69\n",
      "\t\tInput split bytes=316\n",
      "\t\tMap input records=20\n",
      "\t\tMap output bytes=430\n",
      "\t\tMap output materialized bytes=482\n",
      "\t\tMap output records=20\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=306769920\n",
      "\t\tPeak Map Virtual memory (bytes)=2566696960\n",
      "\t\tPeak Reduce Physical memory (bytes)=203268096\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2572869632\n",
      "\t\tPhysical memory (bytes) snapshot=814841856\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=20\n",
      "\t\tReduce output records=1\n",
      "\t\tReduce shuffle bytes=482\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=40\n",
      "\t\tTotal committed heap usage (bytes)=843579392\n",
      "\t\tVirtual memory (bytes) snapshot=7705919488\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/ligagoals.root.20240120.112710.268554/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/ligagoals.root.20240120.112710.268554/output...\n",
      "\"Real Madrid vs Alaves\"\t\"Diferencia de goles 49\"\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/ligagoals.root.20240120.112710.268554...\n",
      "Removing temp directory /tmp/ligagoals.root.20240120.112710.268554...\n"
     ]
    }
   ],
   "source": [
    "! python3 ligagoals.py -r hadoop laliga2122.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.- Calcula la racha de los últimos cinco partidos de cada equipo en la clasificación final de La Liga en la temporada 2021/2022.\n",
    "\n",
    "[Observa](https://www.google.com/search?q=clasificacion+liga+2021+2022&oq=clasificacion+liga+2021+2022#sie=lg) que las últimas columnas de la clasificación muestran cuál ha sido el resultado de los últimos 5 partidos de cada equipo.\n",
    "\n",
    "![clasificacion](./img/clasificacion.png)\n",
    "\n",
    "Se trata de que muestres la clasificación final junto con los resultados de los últimos 5 partidos. Este ejercicio es un poco más difícil y laborioso que los otros. Si usas `mrjob` probablemente te sea útil utilizar [ordenación secundaria por valor](https://mrjob.readthedocs.io/en/latest/job.html#secondary-sort), aunque también se puede resolver sin hacer uso de ella.\n",
    "\n",
    "Se espera este resultado:\n",
    "\n",
    "![solución 5](./img/5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing laliga5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile laliga5.py\n",
    "#!/usr/bin/python3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from datetime import datetime\n",
    "\n",
    "class LaLigaRecentPerformance(MRJob):\n",
    "\n",
    "    SORT_VALUES = True\n",
    "\n",
    "    def mapper_score(self, _, line):\n",
    "        _, date, _, home_team, away_team, _, _, result, *rest = line.split(',')\n",
    "        \n",
    "        if home_team == \"HomeTeam\":\n",
    "            return\n",
    "\n",
    "        date = datetime.strptime(date, \"%d/%m/%Y\").strftime(\"%Y/%m/%d\")\n",
    "\n",
    "        if result == 'D':            \n",
    "            yield home_team, (date, 1)\n",
    "            yield away_team, (date, 1)\n",
    "        elif result == 'H':\n",
    "            yield home_team, (date, 3)\n",
    "            yield away_team, (date, 0)\n",
    "        else:\n",
    "            yield home_team, (date, 0)\n",
    "            yield away_team, (date, 3)\n",
    "\n",
    "    def reducer_score(self, team, scores):\n",
    "        scores = list(scores)\n",
    "        scores = [s for date, s in scores]\n",
    "        recent_five_scores = scores[-5:]\n",
    "        recent_five_scores.reverse()\n",
    "        yield None, (team, sum(scores), recent_five_scores)\n",
    "\n",
    "    def reducer_ranking(self, _, scores):\n",
    "        yield None, sorted(scores, key=lambda t: t[1], reverse=True)\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_score, reducer=self.reducer_score),\n",
    "            MRStep(reducer=self.reducer_ranking)\n",
    "        ]\n",
    "\n",
    "if __name__=='__main__':\n",
    "    LaLigaRecentPerformance.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/laliga5.root.20240120.113849.267838\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/laliga5.root.20240120.113849.267838/output\n",
      "Streaming final output from /tmp/laliga5.root.20240120.113849.267838/output...\n",
      "null\t[[\"Real Madrid\", 86, [1, 1, 3, 0, 3]], [\"Barcelona\", 73, [0, 1, 3, 3, 3]], [\"Ath Madrid\", 71, [3, 1, 3, 3, 0]], [\"Sevilla\", 70, [3, 1, 1, 1, 1]], [\"Betis\", 65, [1, 3, 3, 0, 1]], [\"Sociedad\", 62, [0, 3, 3, 0, 1]], [\"Villarreal\", 59, [3, 0, 3, 1, 0]], [\"Ath Bilbao\", 55, [0, 3, 0, 1, 3]], [\"Valencia\", 48, [3, 1, 0, 1, 1]], [\"Osasuna\", 47, [0, 0, 1, 1, 1]], [\"Celta\", 46, [0, 3, 0, 3, 1]], [\"Elche\", 42, [3, 0, 0, 0, 1]], [\"Espanol\", 42, [1, 1, 0, 1, 0]], [\"Vallecano\", 42, [0, 0, 0, 1, 1]], [\"Cadiz\", 39, [3, 1, 0, 3, 1]], [\"Getafe\", 39, [0, 1, 1, 1, 1]], [\"Mallorca\", 39, [3, 3, 1, 0, 0]], [\"Granada\", 38, [1, 0, 3, 3, 1]], [\"Levante\", 35, [3, 3, 0, 3, 1]], [\"Alaves\", 31, [0, 0, 3, 0, 3]]]\n",
      "Removing temp directory /tmp/laliga5.root.20240120.113849.267838...\n"
     ]
    }
   ],
   "source": [
    "! python3 laliga5.py laliga2122.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /app/hadoop-3.3.1/bin...\n",
      "Found hadoop binary: /app/hadoop-3.3.1/bin/hadoop\n",
      "Using Hadoop version 3.3.1\n",
      "Looking for Hadoop streaming jar in /app/hadoop-3.3.1...\n",
      "Found Hadoop streaming jar: /app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar\n",
      "Creating temp directory /tmp/laliga5.root.20240120.113859.855856\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/laliga5.root.20240120.113859.855856/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/laliga5.root.20240120.113859.855856/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar5213089982492104415/] [] /tmp/streamjob139541007877437592.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.21.0.3:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.21.0.3:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1705747935772_0007\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1705747935772_0007\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1705747935772_0007\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1705747935772_0007/\n",
      "  Running job: job_1705747935772_0007\n",
      "  Job job_1705747935772_0007 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1705747935772_0007 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/laliga5.root.20240120.113859.855856/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=176270\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=770\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=23946\n",
      "\t\tFILE: Number of bytes written=883814\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=176568\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=770\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3047424\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1446912\n",
      "\t\tTotal time spent by all map tasks (ms)=2976\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=2976\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1413\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1413\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=2976\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1413\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=880\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=71\n",
      "\t\tInput split bytes=298\n",
      "\t\tMap input records=381\n",
      "\t\tMap output bytes=22420\n",
      "\t\tMap output materialized bytes=23952\n",
      "\t\tMap output records=760\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=308252672\n",
      "\t\tPeak Map Virtual memory (bytes)=2570514432\n",
      "\t\tPeak Reduce Physical memory (bytes)=201273344\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2570694656\n",
      "\t\tPhysical memory (bytes) snapshot=816570368\n",
      "\t\tReduce input groups=760\n",
      "\t\tReduce input records=760\n",
      "\t\tReduce output records=20\n",
      "\t\tReduce shuffle bytes=23952\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=1520\n",
      "\t\tTotal committed heap usage (bytes)=843055104\n",
      "\t\tVirtual memory (bytes) snapshot=7707222016\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar1992212613717646677/] [] /tmp/streamjob5194881040984448470.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.21.0.3:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.21.0.3:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1705747935772_0008\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1705747935772_0008\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1705747935772_0008\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1705747935772_0008/\n",
      "  Running job: job_1705747935772_0008\n",
      "  Job job_1705747935772_0008 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1705747935772_0008 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/laliga5.root.20240120.113859.855856/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1155\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=696\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=836\n",
      "\t\tFILE: Number of bytes written=837339\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1467\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=696\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3049472\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1401856\n",
      "\t\tTotal time spent by all map tasks (ms)=2978\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=2978\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1369\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1369\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=2978\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1369\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=850\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=78\n",
      "\t\tInput split bytes=312\n",
      "\t\tMap input records=20\n",
      "\t\tMap output bytes=790\n",
      "\t\tMap output materialized bytes=842\n",
      "\t\tMap output records=20\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=308826112\n",
      "\t\tPeak Map Virtual memory (bytes)=2566922240\n",
      "\t\tPeak Reduce Physical memory (bytes)=206708736\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2571431936\n",
      "\t\tPhysical memory (bytes) snapshot=823181312\n",
      "\t\tReduce input groups=20\n",
      "\t\tReduce input records=20\n",
      "\t\tReduce output records=1\n",
      "\t\tReduce shuffle bytes=842\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=40\n",
      "\t\tTotal committed heap usage (bytes)=843579392\n",
      "\t\tVirtual memory (bytes) snapshot=7705251840\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/laliga5.root.20240120.113859.855856/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/laliga5.root.20240120.113859.855856/output...\n",
      "null\t[[\"Real Madrid\", 86, [1, 1, 3, 0, 3]], [\"Barcelona\", 73, [0, 1, 3, 3, 3]], [\"Ath Madrid\", 71, [3, 1, 3, 3, 0]], [\"Sevilla\", 70, [3, 1, 1, 1, 1]], [\"Betis\", 65, [1, 3, 3, 0, 1]], [\"Sociedad\", 62, [0, 3, 3, 0, 1]], [\"Villarreal\", 59, [3, 0, 3, 1, 0]], [\"Ath Bilbao\", 55, [0, 3, 0, 1, 3]], [\"Valencia\", 48, [3, 1, 0, 1, 1]], [\"Osasuna\", 47, [0, 0, 1, 1, 1]], [\"Celta\", 46, [0, 3, 0, 3, 1]], [\"Elche\", 42, [3, 0, 0, 0, 1]], [\"Espanol\", 42, [1, 1, 0, 1, 0]], [\"Vallecano\", 42, [0, 0, 0, 1, 1]], [\"Cadiz\", 39, [3, 1, 0, 3, 1]], [\"Getafe\", 39, [0, 1, 1, 1, 1]], [\"Mallorca\", 39, [3, 3, 1, 0, 0]], [\"Granada\", 38, [1, 0, 3, 3, 1]], [\"Levante\", 35, [3, 3, 0, 3, 1]], [\"Alaves\", 31, [0, 0, 3, 0, 3]]]\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/laliga5.root.20240120.113859.855856...\n",
      "Removing temp directory /tmp/laliga5.root.20240120.113859.855856...\n"
     ]
    }
   ],
   "source": [
    "! python3 laliga5.py -r hadoop laliga2122.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
